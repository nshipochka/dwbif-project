# Описание на задачата 
Разглеждаме банка, която предлага услуги на частни лица. Услугите включват управление на акаунти, предлагане на заеми и други. Банката иска да подобри услугите си, като намери интересни групи клиенти (например да разграничи добрите от лошите клиенти). Мениджърите на банката имат само бегла представа кой е “добър” клиент (на когото да предложат някои допълнителни услуги) и кой е “лош” клиент (когото да наблюдават внимателно, за да намалят загубите на банката). За тази цел банката съхранява данни за своите клиенти: сметки (транзакции в рамките на няколко месеца), вече отпуснати кредити, издадени кредитни карти. Поради това мениджърите на банката търсят отговори на текущите въпроси, като анализират наличните данни.
Задачата е да се дефинира проблем, който може да помогне на банката да подобри своите услуги. От гледна точка на KDD проблемът може да бъде класификация, прогноза или описание и да покажем как KDD може да се използва за решаване на проблема (дори ако резултатите не са много значими от гледна точка на KDD, те могат да бъдат важни за банката).
В идеалния случай всеки подход би трябвало да включва:
1. Предложени цели
2. Подробности за извличане на данни
3. Демонстрирано използване на резултатите
Тъй като резултатите от откриването могат да бъдат неочаквани, приложенията могат да се различават от първоначално предложените.

# Описание на данните:
релация account (4500 обекта във файла ACCOUNT.ASC) - всеки запис описва статични характеристики на акаунт
релация client (5369 обекта във файла CLIENT.ASC) - всеки запис описва характеристики на клиент
релация disposition (5369 обекта във файла DISP.ASC) - всеки запис свързва клиент с акаунт
релация permanent order (6471 обекта във файла ORDER.ASC) - всеки запис описва характеристики на платежно нареждане
релация transaction (1056320 обекта във файла TRANS.ASC) - всеки запис описва една транзакция по сметка
релация loan (682 обекта във файла LOAN.ASC) - всеки запис описва заем, предоставен за дадена сметка
релация credit card (892 обекта във файла CARD.ASC) - всеки запис описва кредитна карта, издадена към сметка
релация demographic data (77 обекта във файла DISTRICT.ASC) - всеки запис описва демографски характеристики на област.


# Обосновка на избрания подход за изграждане на Data Warehouse 
За реализацията на проекта е избран подходът на Ralph Kimball, известен още като подход Bottom-Up. Фокусът на подхода Kimball е върху идентифицирането на бизнес процеса и последващите бизнес решения, които трябва да се предоставят с data warehouse-a. Подходът на Kimball използва dimensional модели (star схема , snowflake схема), за да организира данните, с цел бързото изпълнение на бизнес процесите.
Една от причините за избора на именно този подход е, че методът е интуитивен и лесен за дизайн и разработка. Също така, моделът на Kimball е лесен за разбиране както от технически, така и от нетехнически лица. За разлика от архитектурата на Inmon, подходът “от долу нагоре” на Kimball прави интеграцията на данни подходяща за нуждите на бизнеса.  Ключово в избора на горепосочения подход е факта, че се нуждае от по-малко инженери с по-малко технически умения, за да настроят и поддържат data warehouse-а сравнение с подхода на Inmon “Top-down” (от горе надолу).


# Модел на входните данни (Source data model)
### Order
PK: order_id (INTEGER)
FK: Account_account_id (INTEGER)
bank_to (VARCHAR(2))
account_to (VARCHAR(20))
amount (NUMBER(10, 2))
k_symbol (VARCHAR(8))

### Loan
PK: loan_id (INTEGER)
FK: Account_account_id (INTEGER)
date (DATE)
amount (NUMBER(10, 2))
duration (INTEGER)
payments (NUMBER(10, 2))
status (VARCHAR(1))

### Transaction
PK: trans_id (INTEGER)
FK: Account_account_id (INTEGER)
date_of_trans (DATE)
type (VARCHAR(6))
operation (VARCHAR(16))
amount (NUMBER(15, 2))
balance (NUMBER(15, 2))
k_symbol (VARCHAR(12))
bank (VARCHAR(2))
account (VARCHAR(20))

### Account
PK: account_id (INTEGER)
FK: Demographic_data_district_id (INTEGER)
frequency (VARCHAR(20))
date_of_creating (DATE)

### Demographic_data
PK: district_id (INTEGER)
district_name (DATE)
region (NUMBER(10, 2))
nr_inhabitants (INTEGER)
nr_municipalities_less_499 (INTEGER)
nr_municipalities_500_1999 (INTEGER)
nr_municipalities_2000_9999 (INTEGER)
nr_municipalities_more_10000 (INTEGER)
nr_cities (INTEGER)
ratio_urban_inhab (NUMBER(5, 2))
average_salary (NUMBER(9, 2))
unemp_rate_95 (NUMBER(5, 2))
unemp_rate_96 (NUMBER(5, 2))
nr_enterpr_per_1000_inhab (INTEGER)
nr_commited_crimes_95 (INTEGER)
nr_commited_crimes_96 (INTEGER)

### Client
PK: client_id (INTEGER)
FK: Demographic_data_district_id (INTEGER)
birth_number (DATE)



### Disposition
PK: disp_id (INTEGER)
FK: Client_client_id (INTEGER)
FK: Account_account_id (INTEGER)
type (VARCHAR(6))

### Credit_card
PK: card_id (INTEGER)
FK: Disposition_disp_id (INTEGER)
type_of_card (CHAR(8))
issued DATE


# Връзки:
Account – Order /One-to-Many/  
Account – Loan /One-to-Many/  
Account – Transaction /One-to-Many/  
Account – Disposition /One-to-Many/  
Account – Demographic_data /Many-to-One/
Demographic_data – Client /One-to-Many/  
Disposition – Credit_card /One-to-Many/  
Disposition – Client  /Many-to-One/

# Staging area модел на данните 
Зоната за съхранение на данни в складовете от данни представлява временно хранилище, където се пазят копия на данните от главната база. В Data Warehouse-a архитектурата тази зона се изисква основно с цел бързодействие на системата. Накратко всички необходими данни трябва да бъдат налични преди да могат да се интегрират в склада от данни. Поради наличието на променящи се бизнес цикли, цикли за обработка на данни, хардуерни и мрежови ресурсни ограничения и географски фактори, не е изпълнимо извличането на всички данни от всички операционни бази точно по едно и също време.


### dim_order_type
PK: order_type_id (INTEGER)
type_of_order (VARCHAR(2))

### dim_credit_card
PK: card_id (INTEGER)
issued (DATE)
card_type (CHAR(8))

### dim_account_details
PK: account_id (INTEGER)
frequency (VARCHAR(20))

### dim_transaction_info
PK: trans_id (INTEGER)
type (VARCHAR(8))
operation (VARCHAR(15))
amount (NUMBER(10, 2))

### dim_date
PK: date_id (TIMESTAMP)
day_number (INTEGER)
day_of_week_name (VARCHAR(10))
month_number (INTEGER)
month_name (VARCHAR(9))
year (INTEGER)
is_working_day (CHAR(1))
week_number (INTEGER)

# DWH модел на данните
Моделирането на складовете от данни е процес на проектиране на схемите както на детайлната, така и на обобщената информация в хранилището от данни. Целта на моделирането е да се разработи схема, описваща реалността или поне част от нея, която складът за данни трябва да поддържа. Това е съществен етап от изграждането на склад от данни поради две основни причини. Първо, чрез схемата клиентите на склада от данни могат да визуализират връзките между данните в склада, за да ги използват по-лесно. Второ, добре разработената схема позволява да се създаде ефективна структура на хранилището за данни, която да помогне за намаляване на разходите за внедряване на хранилището и да подобри ефективността при използването му.    

Основната функция на хранилищата от данни е да подпомагат процесите на DSS (Decision support system). Следователно целта на моделирането на хранилището е да се направи така, че то ефективно да поддържа сложни заявки за дългосрочна информация. За разлика от това, моделирането на данни в оперативните системи за бази данни цели ефективна поддръжка на прости транзакции. Освен това складовете за данни са предназначени за клиенти с общи познания за информацията в предприятието, докато оперативните системи за бази данни са по-скоро ориентирани към използване от софтуерни специалисти за създаване на отделни приложения.    

За целта се изготвят два вида таблици - таблици на фактите (fact) и таблици на измеренията (dimensional). Таблицата с факти е централната таблица в схемата от тип “звезда” в даден склад от данни. Тя съхранява количествена информация за анализ и често е денормализирана. Fact таблицата работи с таблици с измерения. Тя съхранява данните, които трябва да се анализират, а dimensional таблицата съхранява данни за начините, по които могат да се анализират данните във fact таблицата. По този начин таблицата с факти се състои от два вида колони. Колони с чужди ключове (foreign keys) позволяващи свързване с таблици на измеренията и колоните за измервания, съдържащи данните, които се анализират.

### Fact_gold_cards_periods
PK,FK: dim_credit_card_id (INTEGER)
count_gold_cards (INTEGER)
date_of_count (DATE)

### Fact_order_payment_type
PK,FK: dim_order_type_id (INTEGER)
count_pojistne_type (INTEGER)
count_sipo_type (INTEGER)
count_leasing_type (INTEGER)
count_uver_type (INTEGER)

### Fact_gold_cards_periods
PK,FK: account_details_id (INTEGER)
PK,FK: dim_transaction_info_id (INTEGER)
count_prijem_type (INTEGER)
count_vydaj_type (INTEGER)





# Описание на ETL процеса
ETL или Extract Transform Load процеса се състои от извличане, трансформиране и зареждане на данни. По време на извличането на данни необработените данни,които могат да бъдат структурирани или неструктурирани, се копират или експортират от местата на източника в зона за съхранение. При трансформирането суровите данни се подлагат на обработка (т. 5 DWH модел на данните). В тази стъпка данните се трансформират и консолидират за предвидената аналитична употреба. При последната стъпка (зареждане) трансформираните данни се преместват от зоната за съхранение в целевия склад за данни. Обикновено това включва първоначално зареждане на всички данни, последвано от периодично зареждане на инкрементални промени в данните, като по-рядко следва пълно опресняване за изтриване и замяна на данните в склада.     

В конкретния случай PKDD’99 е единственият източник на данни.  Всички ETL процеси се изпълняват по едно и също време – данните се качват/обновяват само веднъж, а не периодично.

# Примерни изводи от данните 


От направените справки може да заключим, че, плащания тип “POJISTNE” (Застрахователни плащания) е най-честата използвана услуга, а най-малко използваната е тип “UVER” (плащане на кредит). За напред можем да разпространим повече останалите предлагани услуги.  


От направените справки може да заключим, че за кратък период от време златните карти са се увеличили почти 10 пъти. Клиентите се интересуват все повече от тази предоставена услуга и за в бъдеще може да се помисли в тази насока за предоставянето на повече златни карти с по-добри условия.

# Използван софтуер
VS Code - ETL процеси
Oracle Data Modeler - DWH модел на данните, Модел на входните данни (Source data model), Staging area модел на данните
DataGrip - генериране на таблиците и извеждане на заявките

# Източници
Презентациите от лекции
https://sorry.vse.cz/~berka/challenge/pkdd1999/berka.htm
https://www.javatpoint.com/
https://www.techtarget.com/
https://www.guru99.com/
https://data-warehouses.net/.
https://www.ibm.com/
